{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Saving and Loading Models weights and parameters.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7zu8KC8UnsgC","colab_type":"text"},"source":["# Saving and Loading Models\n","\n","In this notebook, I'll  save and load models with PyTorch. This is important because we often want to load previously trained models to use in making predictions or to continue training on new data."]},{"cell_type":"code","metadata":{"id":"9J7vG8YYnsgD","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","import helper\n","import fc_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wci4fHiznsgG","colab_type":"code","outputId":"2e45be71-a440-47d2-bfbe-b89a8f9669e4","colab":{}},"source":["# Defining a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,))])\n","# Downloading and loading the training data\n","trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","# Downloading and loading the test data\n","testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1bfuStvRnsgL","colab_type":"text"},"source":["Here we can see one of the images."]},{"cell_type":"code","metadata":{"id":"I1qrv8I-nsgM","colab_type":"code","outputId":"5a9b1279-6f06-4ea8-9d3d-a88e3820d973","colab":{}},"source":["image, label = next(iter(trainloader))\n","helper.imshow(image[0,:]);"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<matplotlib.figure.Figure at 0x7f5553f1ad68>"]},"metadata":{"tags":[],"image/png":{"height":233,"width":233},"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"wAB7MqfYnsgQ","colab_type":"text"},"source":["# Train a network\n","\n","To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to show how we can save and load models."]},{"cell_type":"code","metadata":{"id":"K5m85MTjnsgR","colab_type":"code","colab":{}},"source":["# Creating the network, defining the criterion and optimizer\n","\n","model = fc_model.Network(784, 10, [512, 256, 128])\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe_1sRJznsgT","colab_type":"code","outputId":"cc3b6ac0-c56f-4e54-f8ad-5f7030378553","colab":{}},"source":["fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1/2..  Training Loss: 1.688..  Test Loss: 0.974..  Test Accuracy: 0.637\n","Epoch: 1/2..  Training Loss: 1.065..  Test Loss: 0.746..  Test Accuracy: 0.708\n","Epoch: 1/2..  Training Loss: 0.853..  Test Loss: 0.668..  Test Accuracy: 0.742\n","Epoch: 1/2..  Training Loss: 0.819..  Test Loss: 0.635..  Test Accuracy: 0.743\n","Epoch: 1/2..  Training Loss: 0.752..  Test Loss: 0.663..  Test Accuracy: 0.745\n","Epoch: 1/2..  Training Loss: 0.721..  Test Loss: 0.608..  Test Accuracy: 0.773\n","Epoch: 1/2..  Training Loss: 0.703..  Test Loss: 0.594..  Test Accuracy: 0.786\n","Epoch: 1/2..  Training Loss: 0.661..  Test Loss: 0.569..  Test Accuracy: 0.782\n","Epoch: 1/2..  Training Loss: 0.660..  Test Loss: 0.598..  Test Accuracy: 0.787\n","Epoch: 1/2..  Training Loss: 0.642..  Test Loss: 0.548..  Test Accuracy: 0.801\n","Epoch: 1/2..  Training Loss: 0.630..  Test Loss: 0.538..  Test Accuracy: 0.801\n","Epoch: 1/2..  Training Loss: 0.610..  Test Loss: 0.515..  Test Accuracy: 0.810\n","Epoch: 1/2..  Training Loss: 0.613..  Test Loss: 0.524..  Test Accuracy: 0.808\n","Epoch: 1/2..  Training Loss: 0.622..  Test Loss: 0.513..  Test Accuracy: 0.809\n","Epoch: 1/2..  Training Loss: 0.594..  Test Loss: 0.506..  Test Accuracy: 0.817\n","Epoch: 1/2..  Training Loss: 0.636..  Test Loss: 0.511..  Test Accuracy: 0.810\n","Epoch: 1/2..  Training Loss: 0.589..  Test Loss: 0.511..  Test Accuracy: 0.815\n","Epoch: 1/2..  Training Loss: 0.629..  Test Loss: 0.495..  Test Accuracy: 0.822\n","Epoch: 1/2..  Training Loss: 0.589..  Test Loss: 0.482..  Test Accuracy: 0.827\n","Epoch: 1/2..  Training Loss: 0.566..  Test Loss: 0.496..  Test Accuracy: 0.824\n","Epoch: 1/2..  Training Loss: 0.584..  Test Loss: 0.490..  Test Accuracy: 0.821\n","Epoch: 1/2..  Training Loss: 0.572..  Test Loss: 0.481..  Test Accuracy: 0.820\n","Epoch: 1/2..  Training Loss: 0.527..  Test Loss: 0.467..  Test Accuracy: 0.827\n","Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.487..  Test Accuracy: 0.819\n","Epoch: 2/2..  Training Loss: 0.550..  Test Loss: 0.483..  Test Accuracy: 0.823\n","Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.475..  Test Accuracy: 0.833\n","Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.469..  Test Accuracy: 0.829\n","Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.463..  Test Accuracy: 0.832\n","Epoch: 2/2..  Training Loss: 0.505..  Test Loss: 0.471..  Test Accuracy: 0.823\n","Epoch: 2/2..  Training Loss: 0.543..  Test Loss: 0.468..  Test Accuracy: 0.833\n","Epoch: 2/2..  Training Loss: 0.580..  Test Loss: 0.470..  Test Accuracy: 0.826\n","Epoch: 2/2..  Training Loss: 0.561..  Test Loss: 0.466..  Test Accuracy: 0.829\n","Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.477..  Test Accuracy: 0.825\n","Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.454..  Test Accuracy: 0.831\n","Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.457..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.460..  Test Accuracy: 0.837\n","Epoch: 2/2..  Training Loss: 0.525..  Test Loss: 0.468..  Test Accuracy: 0.830\n","Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.447..  Test Accuracy: 0.835\n","Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.458..  Test Accuracy: 0.836\n","Epoch: 2/2..  Training Loss: 0.491..  Test Loss: 0.456..  Test Accuracy: 0.838\n","Epoch: 2/2..  Training Loss: 0.536..  Test Loss: 0.456..  Test Accuracy: 0.836\n","Epoch: 2/2..  Training Loss: 0.505..  Test Loss: 0.453..  Test Accuracy: 0.834\n","Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.457..  Test Accuracy: 0.831\n","Epoch: 2/2..  Training Loss: 0.557..  Test Loss: 0.440..  Test Accuracy: 0.841\n","Epoch: 2/2..  Training Loss: 0.501..  Test Loss: 0.443..  Test Accuracy: 0.839\n","Epoch: 2/2..  Training Loss: 0.540..  Test Loss: 0.439..  Test Accuracy: 0.840\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j7T319fXnsgX","colab_type":"text"},"source":["## Saving and loading networks\n","\n","As we can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n","\n","The parameters for PyTorch networks are stored in a model's `state_dict`.  "]},{"cell_type":"code","metadata":{"id":"o17qmv4ynsgX","colab_type":"code","outputId":"40e7900c-5f6b-431a-ea6d-a46151fffa2b","colab":{}},"source":["print(\"Our model: \\n\\n\", model, '\\n')\n","print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Our model: \n","\n"," Network(\n","  (hidden_layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=400, bias=True)\n","    (1): Linear(in_features=400, out_features=200, bias=True)\n","    (2): Linear(in_features=200, out_features=100, bias=True)\n","  )\n","  (output): Linear(in_features=100, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.5)\n",") \n","\n","The state dict keys: \n","\n"," odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rCBKotyNnsgb","colab_type":"text"},"source":["we will simply save the state dict with `torch.save` to  save it to a file `'checkpoint.pth'`."]},{"cell_type":"code","metadata":{"id":"wTOeexgJnsgb","colab_type":"code","colab":{}},"source":["torch.save(model.state_dict(), 'checkpoint.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MG0fz8CLnsge","colab_type":"text"},"source":["Then we can load the state dict with `torch.load`."]},{"cell_type":"code","metadata":{"id":"B_Y81MfTnsgg","colab_type":"code","outputId":"af0938e8-fdc1-4ac3-c266-54cdfe984193","colab":{}},"source":["state_dict = torch.load('checkpoint.pth')\n","print(state_dict.keys())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jn9CsPH3nsgk","colab_type":"text"},"source":["And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."]},{"cell_type":"code","metadata":{"id":"M4IhFu7Knsgk","colab_type":"code","colab":{}},"source":["model.load_state_dict(state_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shtIWOnHnsgn","colab_type":"text"},"source":["Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."]},{"cell_type":"code","metadata":{"id":"IJLo0tvHnsgp","colab_type":"code","outputId":"605cfd23-b26f-43a9-92bc-6c908fbd519b","colab":{}},"source":["# Try this\n","model = fc_model.Network(784, 10, [400, 200, 100])\n","# This will throw an error because the tensor sizes are wrong!\n","model.load_state_dict(state_dict)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for Network:\n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([512, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([512]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([256, 512]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"hidden_layers.2.weight\", whose dimensions in the model are torch.Size([100, 200]) and whose dimensions in the checkpoint are torch.Size([128, 256]).\n\tWhile copying the parameter named \"hidden_layers.2.bias\", whose dimensions in the model are torch.Size([100]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"output.weight\", whose dimensions in the model are torch.Size([10, 100]) and whose dimensions in the checkpoint are torch.Size([10, 128]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tWhile copying the parameter named \"hidden_layers.0.weight\", whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([512, 784]).\n\tWhile copying the parameter named \"hidden_layers.0.bias\", whose dimensions in the model are torch.Size([400]) and whose dimensions in the checkpoint are torch.Size([512]).\n\tWhile copying the parameter named \"hidden_layers.1.weight\", whose dimensions in the model are torch.Size([200, 400]) and whose dimensions in the checkpoint are torch.Size([256, 512]).\n\tWhile copying the parameter named \"hidden_layers.1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"hidden_layers.2.weight\", whose dimensions in the model are torch.Size([100, 200]) and whose dimensions in the checkpoint are torch.Size([128, 256]).\n\tWhile copying the parameter named \"hidden_layers.2.bias\", whose dimensions in the model are torch.Size([100]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"output.weight\", whose dimensions in the model are torch.Size([10, 100]) and whose dimensions in the checkpoint are torch.Size([10, 128])."]}]},{"cell_type":"markdown","metadata":{"id":"8TrE-mP2nsgs","colab_type":"text"},"source":["This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this,we will build a dictionary with all the information you need to compeletely rebuild the model."]},{"cell_type":"code","metadata":{"id":"5VoK3FAZnsgt","colab_type":"code","colab":{}},"source":["checkpoint = {'input_size': 784,\n","              'output_size': 10,\n","              'hidden_layers': [each.out_features for each in model.hidden_layers],\n","              'state_dict': model.state_dict()}\n","\n","torch.save(checkpoint, 'checkpoint.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93oe5kfjnsgw","colab_type":"text"},"source":["Now the checkpoint has all the necessary information to rebuild the trained model. we can easily make that a function if we want. Similarly, we can write a function to load checkpoints. "]},{"cell_type":"code","metadata":{"id":"j70ed8gVnsgw","colab_type":"code","colab":{}},"source":["def load_checkpoint(filepath):\n","    checkpoint = torch.load(filepath)\n","    model = fc_model.Network(checkpoint['input_size'],\n","                             checkpoint['output_size'],\n","                             checkpoint['hidden_layers'])\n","    model.load_state_dict(checkpoint['state_dict'])\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpp2RLFrnsgz","colab_type":"code","outputId":"362e5bc1-718e-4b48-820c-b6698212537e","colab":{}},"source":["model = load_checkpoint('checkpoint.pth')\n","print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Network(\n","  (hidden_layers): ModuleList(\n","    (0): Linear(in_features=784, out_features=400, bias=True)\n","    (1): Linear(in_features=400, out_features=200, bias=True)\n","    (2): Linear(in_features=200, out_features=100, bias=True)\n","  )\n","  (output): Linear(in_features=100, out_features=10, bias=True)\n","  (dropout): Dropout(p=0.5)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Co96vO-8nsg2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}