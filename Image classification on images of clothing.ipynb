{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Lets build and train a neural network. we'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, we'll build your own neural network. \n",
    "\n",
    "First off, let's load the dataset through torchvision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here we should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network architecture\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [256,128,64]          \n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0],hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1],hidden_sizes[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now we will create your network and train it. First you'll want to define the criterion and the optmizer\n",
    "\n",
    "Then write the training code.\n",
    "\n",
    "we will later try adjusting the hyperparameters (hidden units, learning rate, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5104099324803109\n",
      "Training loss: 0.3919566776246023\n",
      "Training loss: 0.35131114205794295\n",
      "Training loss: 0.33394703039450685\n",
      "Training loss: 0.3145276668674148\n",
      "Training loss: 0.30008797902764794\n",
      "Training loss: 0.293196404086692\n",
      "Training loss: 0.28359997046908847\n",
      "Training loss: 0.2756675399029687\n",
      "Training loss: 0.26610765772968975\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    for images,labels in trainloader:\n",
    "        images= images.reshape(images.shape[0],-1)\n",
    "        optimizer.zero_grad()  # setting the gradient to 0 for every iteration so that it doesnt retain gradient from previous batches\n",
    "        output= model(images)\n",
    "        Loss= criterion(output,labels)\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+= Loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
